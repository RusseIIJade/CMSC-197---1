{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8085ac8",
   "metadata": {},
   "source": [
    "# HW4 Words Occuring 50 Times Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4439e",
   "metadata": {},
   "source": [
    "Submitted by Russel Jade Tumanon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca17a16",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d3d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "#For Preprocessing and results\n",
    "import email\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e2c2b",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b8b51",
   "metadata": {},
   "source": [
    "## Creating Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3df4053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>data/000/000</td>\n",
       "      <td>Received: from rodan.UU.NET by aramis.rutgers....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/001</td>\n",
       "      <td>Received: from unknown (HELO groucho.cs.psu.ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/002</td>\n",
       "      <td>Received:\\n\\tfrom 24-151-178-89.dhcp.kgpt.tn.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>data/000/003</td>\n",
       "      <td>Received: from psuvax1.cs.psu.edu ([130.203.2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/004</td>\n",
       "      <td>Received: from 201-1-198-159.dsl.telesp.net.br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label          path                                           contents\n",
       "0   ham  data/000/000  Received: from rodan.UU.NET by aramis.rutgers....\n",
       "1  spam  data/000/001  Received: from unknown (HELO groucho.cs.psu.ed...\n",
       "2  spam  data/000/002  Received:\\n\\tfrom 24-151-178-89.dhcp.kgpt.tn.c...\n",
       "3   ham  data/000/003  Received: from psuvax1.cs.psu.edu ([130.203.2....\n",
       "4  spam  data/000/004  Received: from 201-1-198-159.dsl.telesp.net.br..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating table or dataframe to easily view the parts\n",
    "labels_file = []\n",
    "labels = []\n",
    "path = []\n",
    "temp = []\n",
    "email_contents = []\n",
    "\n",
    "#Setting DataFrame\n",
    "emails = pd.DataFrame()\n",
    "\n",
    "#Open labels file\n",
    "with open('labels') as file:\n",
    "    labels_file = file.readlines()\n",
    "\n",
    "#Remove New lines   \n",
    "for i in labels_file:\n",
    "    temp.append(i.replace(\"\\n\", \"\"))\n",
    "    \n",
    "labels_file = temp\n",
    "\n",
    "#Get the label, addr/path, and email contents\n",
    "for i in labels_file:\n",
    "    label, separator, addr = i.partition(\" \")\n",
    "    addr = addr[3:]\n",
    "    path.append(addr)\n",
    "    labels.append(label)\n",
    "\n",
    "    #Put each email content to email_contents\n",
    "    with open(addr, \"r\", errors='ignore') as currFile:\n",
    "        contents = currFile.read()\n",
    "        email_contents.append(contents)\n",
    "        \n",
    "        currFile.close()\n",
    "\n",
    "#Assigning colum names \n",
    "emails[\"label\"] = labels\n",
    "emails[\"path\"] = path\n",
    "emails[\"contents\"] = email_contents\n",
    "\n",
    "#Print first 5 \n",
    "emails.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32911123",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a9b49",
   "metadata": {},
   "source": [
    "### Functions to Format Email Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85f6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function for string emails\n",
    "def email_messg(email_str):\n",
    "    message_str = email.message_from_string(email_str)\n",
    "    mssg = ''\n",
    "    \n",
    "    #Get Payload\n",
    "    try:\n",
    "        for payload in message_str.get_payload():\n",
    "            mssg = payload.get_payload()\n",
    "    except:\n",
    "        mssg = message_str.get_payload()\n",
    "\n",
    "    #Calls the multipart function if it's not a string\n",
    "    if(type(mssg) != str):\n",
    "        mssg = multipart(email_str)\n",
    "    return str(mssg)\n",
    "\n",
    "#Creating a function for multipart emails\n",
    "def multipart(email_content_str):\n",
    "    email_body = email.message_from_string(email_content_str)\n",
    "    content = \"\"\n",
    "    \n",
    "    #Get the body for multipart emails\n",
    "    if email_body.is_multipart():\n",
    "        \n",
    "        #Get the content type and disposition\n",
    "        for i in email_body.walk():\n",
    "            content_type = i.get_content_type()\n",
    "            content_dispo = str(i.get('Content-Disposition'))\n",
    "            \n",
    "            #Will skip if the content type is text/plain and content disposition do not contain attachment\n",
    "            if content_type == 'text/plain' and 'attachment' not in content_dispo:\n",
    "                \n",
    "                #Decode payload\n",
    "                content = i.get_payload(decode=True)\n",
    "                break\n",
    "                \n",
    "    #If not multipart\n",
    "    else:\n",
    "        \n",
    "        #Decode payload\n",
    "        content = email_body.get_payload(decode=True)\n",
    "    try:\n",
    "        \n",
    "         #Decode content\n",
    "        content = content.decode()\n",
    "    except:\n",
    "        try:\n",
    "            \n",
    "            #Decode content 'latin-1'\n",
    "            content = content.decode('latin-1')\n",
    "        except:\n",
    "            pass\n",
    "    return(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b6a14",
   "metadata": {},
   "source": [
    "### Store all Formatted Email Contents to a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3ac52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Result Sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The mailing list I queried about a few weeks ago is now up and\\nrunning.  I have also set up an archive server; see below.  The\\nfollowing is the official welcome to the list message at the moment.\\n\\nJoe Buehler\\n\\n--------------------\\n\\nThis mailing list is for people who desire serious, orthodox\\ndiscussion of the Roman Catholic religion.  I assume that it will\\ncater mainly to Catholics, but everyone else is welcome, provided they\\noperate within the below guidelines.  My own interests have a\\ndoctrinal bent, but I\\'m certainly not going to limit this list to just\\nthat sort of discussion.\\n\\nHaving participated in USENET religion groups for about 5 years now,\\none of my primary observations about Catholics on the net is that they\\ndo not know their religion very well.  My hope is that this list might\\nhelp remedy this problem to some extent.  I would like to make this a\\nnet resource available for Catholics who want to know more about their\\nreligion.\\n\\nAs far as moderation policy goes: The Catholic Church is not a\\ndemocracy, it\\'s a monarchy subject to a divinely given constitution.\\nI don\\'t set the rules in the Church, neither does my parish priest,\\nnor does my bishop, nor does the Pope.  Everyone has to adhere to the\\nway Christ set things up.  I think it follows that it\\'s not really\\nappropriate for someone to call himself a Catholic and argue with this\\nstate of affairs.  If you want to be Catholic, it\\'s simple enough, you\\nhave to follow the teaching of the Church.\\n\\nThe moderation policy will reflect this way of thinking: there are\\nplenty of other places on the net where Catholic doctrine can be\\nfreely attacked!  If in doubt, you can always subscribe, and see\\nwhether the list is to your taste.\\n\\nBesides the mailing list, there are a few other things that may be of\\ninterest:\\n\\n- I have set up an archive server that we can put interesting things\\nin.  (It doesn\\'t have anything in it at the moment except some UNIX\\nsoftware useful for such endeavors.)  Sorry, guys, but nothing that\\'s\\ncopyrighted goes into it!\\n\\n- I am planning on setting up a quotation server that can email\\nperiodic interesting citations from the principal sources of Catholic\\ndoctrine.  (Not done yet.)\\n\\n- I have obtained permission from the English language publishers of\\nthe Italian Catholic magazine \"30 Days\" to take material from their\\nmagazine.  I intend to scan some of the more interesting pictures\\n(ever seen a Cristero?  or St. Pius X?) and put them in the archives,\\nand also post extracts from some of the more interesting articles.\\n(It\\'s a European magazine, and is of generally higher quality than\\nAmerican Catholic material.  In my opinion...)\\n\\nIf you have any questions, would like to subscribe, etc., send email\\nto\\n\\n\\tcatholic@sarto.budd-lake.nj.us\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store all contents to email_contents\n",
    "email_contents = []\n",
    "for i in emails.contents:\n",
    "    email_contents.append(email_messg(i))\n",
    "\n",
    "#Sample\n",
    "print(f\"Formatted Result Sample:\")\n",
    "email_contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c65efe",
   "metadata": {},
   "source": [
    "### Open Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d36e96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'about', 'above', 'abst']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open the stop_words.txt file\n",
    "with open('stop_words.txt', 'r') as file:\n",
    "\n",
    "    stop_words = []\n",
    "\n",
    "    for line in file:\n",
    "        \n",
    "        #Split the line into a list of words\n",
    "        words = line.split()\n",
    "        \n",
    "        # Add the words to stop_words\n",
    "        stop_words.extend(words)\n",
    "\n",
    "#Print the first 5 stop words\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec7599",
   "metadata": {},
   "source": [
    "### Functions to  Clean the Email Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b78217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Result Sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mailing list queried weeks ago running set archive server official list message moment joe buehler mailing list people desire serious orthodox discussion roman catholic religion assume will cater catholics provided operate guidelines interests doctrinal bent going limit list sort discussion participated usenet religion groups years primary observations catholics net religion well hope list help remedy problem extent net resource catholics religion moderation policy catholic church democracy monarchy subject divinely constitution don set rules church parish priest bishop pope adhere christ set things appropriate call catholic argue affairs catholic simple follow teaching church moderation policy will reflect thinking plenty places net catholic doctrine freely attacked doubt subscribe list taste mailing list things interest set archive server interesting things doesn moment unix software endeavors guys copyrighted planning setting quotation server email periodic interesting citations principal sources catholic doctrine permission english language publishers italian catholic magazine days material magazine intend scan interesting pictures cristero st pius archives post extracts interesting articles european magazine generally higher quality american catholic material opinion questions subscribe send email catholic sarto budd lake nj'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a function to clean email contents\n",
    "def clean(email_content_str):\n",
    "\n",
    "    #Lower case all words\n",
    "    email_content_str = email_content_str.lower().split()\n",
    "    #Joining to string\n",
    "    email_content_str = ' '.join(email_content_str)   \n",
    "    \n",
    "    #Removing non-alphanumeric characters\n",
    "    email_content_str = re.sub(r'[^a-zA-Z0-9]', ' ', email_content_str)\n",
    "\n",
    "    #Removing numbers\n",
    "    email_content_str = re.sub('\\d+', ' ', email_content_str)\n",
    "    \n",
    "    #Removing website links\n",
    "    email_content_str = re.sub(r'\\bhttp\\S*\\b', ' ', email_content_str)\n",
    "    email_content_str = re.sub(r'\\bwww\\S*', ' ', email_content_str)\n",
    "    \n",
    "    #Remove the stop_words\n",
    "    email_content_str = email_content_str.split()\n",
    "    email_content_str = [word for word in email_content_str if word not in stop_words]\n",
    "    email_content_str = \" \".join(email_content_str)\n",
    "\n",
    "    #Remove the stop_words again because some still showed in the unique words table\n",
    "    email_content_str = email_content_str.split()\n",
    "    email_content_str = [word for word in email_content_str if word not in stop_words]\n",
    "    email_content_str = \" \".join(email_content_str)\n",
    "    \n",
    "    #Return cleaned email\n",
    "    return(email_content_str)\n",
    "\n",
    "#Creating a function to clean evey email content\n",
    "def clean_emails(email_contents):\n",
    "    cleaned_emails = []\n",
    "    for email_content_str in email_contents:\n",
    "        cleaned_emails.append(clean(email_content_str))\n",
    "    \n",
    "    return cleaned_emails\n",
    "\n",
    "cleaned_emails = clean_emails(email_contents)\n",
    "\n",
    "#Sample\n",
    "print(f\"Cleaned Result Sample:\")\n",
    "cleaned_emails[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eba549",
   "metadata": {},
   "source": [
    "### Update DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3199a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>contents</th>\n",
       "      <th>cleaned_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>data/000/000</td>\n",
       "      <td>Received: from rodan.UU.NET by aramis.rutgers....</td>\n",
       "      <td>mailing list queried weeks ago running set arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/001</td>\n",
       "      <td>Received: from unknown (HELO groucho.cs.psu.ed...</td>\n",
       "      <td>luxury watches buy rolex rolex cartier bvlgari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/002</td>\n",
       "      <td>Received:\\n\\tfrom 24-151-178-89.dhcp.kgpt.tn.c...</td>\n",
       "      <td>academic qualifications prestigious acc redite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>data/000/003</td>\n",
       "      <td>Received: from psuvax1.cs.psu.edu ([130.203.2....</td>\n",
       "      <td>greetings verify subscription plan fans list c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/004</td>\n",
       "      <td>Received: from 201-1-198-159.dsl.telesp.net.br...</td>\n",
       "      <td>html head meta equiv content language content ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label          path                                           contents  \\\n",
       "0   ham  data/000/000  Received: from rodan.UU.NET by aramis.rutgers....   \n",
       "1  spam  data/000/001  Received: from unknown (HELO groucho.cs.psu.ed...   \n",
       "2  spam  data/000/002  Received:\\n\\tfrom 24-151-178-89.dhcp.kgpt.tn.c...   \n",
       "3   ham  data/000/003  Received: from psuvax1.cs.psu.edu ([130.203.2....   \n",
       "4  spam  data/000/004  Received: from 201-1-198-159.dsl.telesp.net.br...   \n",
       "\n",
       "                                    cleaned_contents  \n",
       "0  mailing list queried weeks ago running set arc...  \n",
       "1  luxury watches buy rolex rolex cartier bvlgari...  \n",
       "2  academic qualifications prestigious acc redite...  \n",
       "3  greetings verify subscription plan fans list c...  \n",
       "4  html head meta equiv content language content ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Update dataframe contents\n",
    "emails[\"cleaned_contents\"] = cleaned_emails\n",
    "emails.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d2d29",
   "metadata": {},
   "source": [
    "## Splitting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffe74f",
   "metadata": {},
   "source": [
    "### Train Set for Ham and Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fffacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Train remember that it has 21,300 emails so we count index 0 to 21300\n",
    "email_train = emails[0:21300]\n",
    "\n",
    "#For ham training set \n",
    "email_train_ham = email_train[email_train['label'].str.contains('ham')]\n",
    "#For spam training set \n",
    "email_train_spam = email_train[email_train['label'].str.contains('spam')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d24c2",
   "metadata": {},
   "source": [
    "### Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44385537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test remember that it has 16,522 emails so we count from train's last index to the total emails + 1 = 37823\n",
    "email_test = emails[21300:37823]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e2f5f",
   "metadata": {},
   "source": [
    "### Checking Emails of Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44f0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (21300, 4)\n",
      "Trainset shape for ham: (7523, 4)\n",
      "Train set shape for spam: (13777, 4)\n",
      "\n",
      "Splitting Train dataset for ham and spam successful!\n",
      "\n",
      "Test set shape: (16522, 4)\n"
     ]
    }
   ],
   "source": [
    "#Check Train Set\n",
    "print(\"Train set shape:\", email_train.shape)\n",
    "print(\"Trainset shape for ham:\", email_train_ham.shape)\n",
    "print(\"Train set shape for spam:\", email_train_spam.shape)\n",
    "\n",
    "#Checks if the number of emails from the training dataset of ham and spam is equal to the total train emails (21300)\n",
    "if (email_train_ham.shape[0] + email_train_spam.shape[0] == 21300):\n",
    "    print(\"\\nSplitting Train dataset for ham and spam successful!\")\n",
    "else:\n",
    "    print(\"Emails do not add up!\")\n",
    "\n",
    "#Check Test Set \n",
    "print(\"\\nTest set shape:\", email_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4115b6",
   "metadata": {},
   "source": [
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c580999",
   "metadata": {},
   "source": [
    "## Occurrence of Unique words (include only words occurring more than 50 times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0629e1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6795</th>\n",
       "      <td>affairs</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6796</th>\n",
       "      <td>matching</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6797</th>\n",
       "      <td>newer</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6798</th>\n",
       "      <td>bsbqzxryb</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6799</th>\n",
       "      <td>xldw</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6893</th>\n",
       "      <td>klcllh</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6894</th>\n",
       "      <td>thumbs</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6895</th>\n",
       "      <td>booster</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>caandagadxqiaa</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>msonormaltable</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  occurence\n",
       "6795         affairs         50\n",
       "6796        matching         50\n",
       "6797           newer         50\n",
       "6798       bsbqzxryb         50\n",
       "6799            xldw         50\n",
       "...              ...        ...\n",
       "6893          klcllh         50\n",
       "6894          thumbs         50\n",
       "6895         booster         50\n",
       "6896  caandagadxqiaa         50\n",
       "6897  msonormaltable         50\n",
       "\n",
       "[103 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the occurrence of each unique word in cleaned_contents using a Counter\n",
    "unique = Counter(\" \".join(email_train['cleaned_contents']).split()).most_common(10000)\n",
    "dictionary = pd.DataFrame(unique, columns = ['word', 'occurence'])\n",
    "\n",
    "dictionary = dictionary[dictionary['occurence'] == 50]\n",
    "\n",
    "dictionary.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455a89a",
   "metadata": {},
   "source": [
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce4c48",
   "metadata": {},
   "source": [
    "# 2. Creating the feature matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb62eac",
   "metadata": {},
   "source": [
    "### For Ham Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731ba4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_list_ham = email_train_ham['cleaned_contents'].tolist()\n",
    "dict_words = dictionary['word'].tolist()\n",
    "\n",
    "#Create empty feature matrix\n",
    "feature_matrix_ham = np.zeros((len(contents_list_ham), len(dict_words)))\n",
    "\n",
    "#Loop over the emails\n",
    "for i, email in enumerate(contents_list_ham):\n",
    "    \n",
    "    #Check if the word in the dictionary appears in the email\n",
    "    for j, word in enumerate(dict_words):\n",
    "        if word in email:\n",
    "            #If it appears, set the value to 1\n",
    "            feature_matrix_ham[i, j] = 1\n",
    "    else:\n",
    "        #Set to 0\n",
    "        feature_matrix_ham[i, j] = 0\n",
    "\n",
    "feature_matrix_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90520ca2",
   "metadata": {},
   "source": [
    "### For Spam Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f7c2e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_list_spam = email_train_spam['cleaned_contents'].tolist()\n",
    "\n",
    "#Create empty feature matrix\n",
    "feature_matrix_spam = np.zeros((len(contents_list_spam), len(dict_words)))\n",
    "\n",
    "#Loop over the emails\n",
    "for i, email in enumerate(contents_list_spam):\n",
    "    \n",
    "    #Check if the word in the dictionary appears in the email\n",
    "    for j, word in enumerate(dict_words):\n",
    "        if word in email:\n",
    "        \n",
    "            #If it appears, set the value to 1\n",
    "            feature_matrix_spam[i, j] = 1\n",
    "    else:\n",
    "        \n",
    "        #Set to 0\n",
    "        feature_matrix_spam[i, j] = 0\n",
    "\n",
    "feature_matrix_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a20c91",
   "metadata": {},
   "source": [
    "# 3. Computing the Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20300e16",
   "metadata": {},
   "source": [
    "### Preparing Variabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e77b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of ham emails in the training set\n",
    "train_size_ham = email_train_ham.shape[0]\n",
    "\n",
    "#The number of spam emails in the training set\n",
    "train_size_spam = email_train_spam.shape[0]\n",
    "\n",
    "#The total number of emails in train set\n",
    "total_email_size = email_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75140f",
   "metadata": {},
   "source": [
    "### Solution for Prior Probabilities of Ham and Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83e0e6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(c = ham) = 0.3531924882629108\n",
      "P(c = spam) = 0.6468075117370892\n"
     ]
    }
   ],
   "source": [
    "#Solution for the prior probabilities for ham\n",
    "prior_ham =  train_size_ham / total_email_size\n",
    "\n",
    "#Solution for the prior probabilities for spam\n",
    "prior_spam = train_size_spam / total_email_size\n",
    "\n",
    "#Showing Results\n",
    "print(f\"P(c = ham) = {prior_ham}\")\n",
    "print(f\"P(c = spam) = {prior_spam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c739977",
   "metadata": {},
   "source": [
    "# 4. Computing the Likelihood of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cf6a1",
   "metadata": {},
   "source": [
    "### Preparing Variabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33dbcce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all the spam words\n",
    "words_train_spam_sum = np.sum(feature_matrix_spam, axis=0)\n",
    "\n",
    "#Add all ham words\n",
    "words_train_ham_sum  = np.sum(feature_matrix_ham, axis=0)\n",
    "\n",
    "#Total sum of spam words\n",
    "words_train_spam_sum_total = words_train_spam_sum.sum()\n",
    "\n",
    "#Total sum of ham words\n",
    "words_train_ham_sum_total = words_train_ham_sum.sum()\n",
    "\n",
    "#For laplace smoothing\n",
    "laplace_smoothing = 1 \n",
    "\n",
    "#Create blank dicts\n",
    "likelihood_ham = {}\n",
    "likelihood_spam = {}\n",
    "#formula based on the given formula in the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57daf3a5",
   "metadata": {},
   "source": [
    "### Solution for the Likelihood of Each Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "135bd687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Sample\n",
      "\n",
      "Likelyhood of \"affairs\" occuring as spam is 0.0004394776494224008.\n",
      "\n",
      "Likelyhood of \"affairs\" occuring as ham is 0.006838422391857507.\n"
     ]
    }
   ],
   "source": [
    "uniq_words = dictionary['word'].tolist()\n",
    "\n",
    "for i in range(len(uniq_words)):\n",
    "\n",
    "    #Getting each word's occurance (spam)\n",
    "    occur_spam = (words_train_spam_sum[i] + laplace_smoothing) / (words_train_spam_sum_total + laplace_smoothing * len(uniq_words))\n",
    "    likelihood_spam[uniq_words[i]] = occur_spam  \n",
    "    \n",
    "    #Getting each word's occurance (ham)\n",
    "    occur_ham = (words_train_ham_sum[i] + laplace_smoothing) / (words_train_ham_sum_total + laplace_smoothing * len(uniq_words))\n",
    "    likelihood_ham[uniq_words[i]] = occur_ham\n",
    "\n",
    "#Sample\n",
    "#likelihood_spam\n",
    "first_key_spam, first_value_spam = next(iter(likelihood_spam.items()))\n",
    "print(f\"Result Sample\\n\\nLikelyhood of \\\"{first_key_spam}\\\" occuring as spam is {first_value_spam}.\")\n",
    "\n",
    "#likelihood_ham\n",
    "first_key_ham, first_value_ham = next(iter(likelihood_ham.items()))\n",
    "print(f\"\\nLikelyhood of \\\"{first_key_ham}\\\" occuring as ham is {first_value_ham}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2694d23",
   "metadata": {},
   "source": [
    "# 5. Classifying the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "757bf7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functiom to classify if it's a spam or ham based on the probability of its word's occurrance\n",
    "contents_list = emails['cleaned_contents'].tolist()\n",
    "def email_classify(prior_spam, prior_ham, contents_list, likelihood_spam, likelihood_ham, uniq_words):\n",
    "    \n",
    "    #Converting the prior probabilities of spam and ham to log probabilities for numerical stability\n",
    "    log_prob_spam = np.log(prior_spam)\n",
    "    log_prob_ham = np.log(prior_ham)\n",
    "    \n",
    "    #Split the cleaned email contents into a list of individual words\n",
    "    body = str(contents_list).split()\n",
    "    \n",
    "    for word in body:\n",
    "\n",
    "        #Update the log probabilities if the word is in the list of unique words\n",
    "        if word in uniq_words:\n",
    "            log_prob_spam += np.log(likelihood_spam[word])            \n",
    "            log_prob_ham += np.log(likelihood_ham[word])\n",
    "\n",
    "    #Check if it is more likely to be spam or ham\n",
    "    if log_prob_spam > log_prob_ham:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098788a",
   "metadata": {},
   "source": [
    "# 6. Testing the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c81b1",
   "metadata": {},
   "source": [
    "### Testing if the predicted label and actual label is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "086c8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test it to all the emails\n",
    "predicted_label = []\n",
    "\n",
    "#Go through each email content and classify them\n",
    "for content in contents_list:\n",
    "    predicted = email_classify(prior_spam, prior_ham, content, likelihood_spam, likelihood_ham, uniq_words)\n",
    "    predicted_label.append(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2657e2",
   "metadata": {},
   "source": [
    "### Add to the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "281cb816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>predicted label</th>\n",
       "      <th>path</th>\n",
       "      <th>contents</th>\n",
       "      <th>cleaned_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>data/000/000</td>\n",
       "      <td>Received: from rodan.UU.NET by aramis.rutgers....</td>\n",
       "      <td>mailing list queried weeks ago running set arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/001</td>\n",
       "      <td>Received: from unknown (HELO groucho.cs.psu.ed...</td>\n",
       "      <td>luxury watches buy rolex rolex cartier bvlgari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/002</td>\n",
       "      <td>Received:\\n\\tfrom 24-151-178-89.dhcp.kgpt.tn.c...</td>\n",
       "      <td>academic qualifications prestigious acc redite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/003</td>\n",
       "      <td>Received: from psuvax1.cs.psu.edu ([130.203.2....</td>\n",
       "      <td>greetings verify subscription plan fans list c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>data/000/004</td>\n",
       "      <td>Received: from 201-1-198-159.dsl.telesp.net.br...</td>\n",
       "      <td>html head meta equiv content language content ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>data/003/095</td>\n",
       "      <td>Received: from mail.oh-oku.com (61-30-232-94.s...</td>\n",
       "      <td>fc da bl ha ha nb kbn pgr ha ej mxmqnal naam b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>data/003/096</td>\n",
       "      <td>Received: from unicorn.acs.ttu.edu (unicorn.ac...</td>\n",
       "      <td>wind erosion list members announcement receive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>ham</td>\n",
       "      <td>ham</td>\n",
       "      <td>data/003/097</td>\n",
       "      <td>Received: from unicorn.acs.ttu.edu (unicorn.ac...</td>\n",
       "      <td>update land damaged wind erosion united presen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>data/003/098</td>\n",
       "      <td>Received: from mail.oh-oku.com (61-30-232-94.s...</td>\n",
       "      <td>fc da bl ha ha nb kbn pgr ha ej mxmqnal naam b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>data/003/099</td>\n",
       "      <td>Received: from bottom.magnus.acs.ohio-state.ed...</td>\n",
       "      <td>rob schuette early work brown period asserts e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label predicted label          path  \\\n",
       "0     ham             ham  data/000/000   \n",
       "1    spam            spam  data/000/001   \n",
       "2    spam            spam  data/000/002   \n",
       "3     ham            spam  data/000/003   \n",
       "4    spam            spam  data/000/004   \n",
       "..    ...             ...           ...   \n",
       "995  spam            spam  data/003/095   \n",
       "996   ham            spam  data/003/096   \n",
       "997   ham             ham  data/003/097   \n",
       "998  spam            spam  data/003/098   \n",
       "999   ham            spam  data/003/099   \n",
       "\n",
       "                                              contents  \\\n",
       "0    Received: from rodan.UU.NET by aramis.rutgers....   \n",
       "1    Received: from unknown (HELO groucho.cs.psu.ed...   \n",
       "2    Received:\\n\\tfrom 24-151-178-89.dhcp.kgpt.tn.c...   \n",
       "3    Received: from psuvax1.cs.psu.edu ([130.203.2....   \n",
       "4    Received: from 201-1-198-159.dsl.telesp.net.br...   \n",
       "..                                                 ...   \n",
       "995  Received: from mail.oh-oku.com (61-30-232-94.s...   \n",
       "996  Received: from unicorn.acs.ttu.edu (unicorn.ac...   \n",
       "997  Received: from unicorn.acs.ttu.edu (unicorn.ac...   \n",
       "998  Received: from mail.oh-oku.com (61-30-232-94.s...   \n",
       "999  Received: from bottom.magnus.acs.ohio-state.ed...   \n",
       "\n",
       "                                      cleaned_contents  \n",
       "0    mailing list queried weeks ago running set arc...  \n",
       "1    luxury watches buy rolex rolex cartier bvlgari...  \n",
       "2    academic qualifications prestigious acc redite...  \n",
       "3    greetings verify subscription plan fans list c...  \n",
       "4    html head meta equiv content language content ...  \n",
       "..                                                 ...  \n",
       "995  fc da bl ha ha nb kbn pgr ha ej mxmqnal naam b...  \n",
       "996  wind erosion list members announcement receive...  \n",
       "997  update land damaged wind erosion united presen...  \n",
       "998  fc da bl ha ha nb kbn pgr ha ej mxmqnal naam b...  \n",
       "999  rob schuette early work brown period asserts e...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert to the right side of label to easily make a comparison\n",
    "emails.insert(1, 'predicted label', predicted_label)\n",
    "emails.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec912e86",
   "metadata": {},
   "source": [
    "### Count how many correct emails were classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a357ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified emails account to 25957/37822 or 68.63%\n"
     ]
    }
   ],
   "source": [
    "#Count rows that are equal\n",
    "count = (emails['label'].eq(emails['predicted label'])).sum()\n",
    "total = len(contents_list)\n",
    "\n",
    "#Convert to Percentage\n",
    "percentage = count/total * 100\n",
    "percentage = round(percentage, 2)\n",
    "\n",
    "print(f\"Correctly classified emails account to {count}/{total} or {percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca938f3",
   "metadata": {},
   "source": [
    "# 7. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4522d0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Evaluation:\n",
      "\n",
      "The Accuracy Score is 0.6862936915023002 or 68.63%\n",
      "The Recall Score is 0.10790085205267234 or 10.79%\n",
      "The Precision Score is 0.8001148765077541 or 80.01%\n"
     ]
    }
   ],
   "source": [
    "#Performance evaluation accuracy, recall, and precision\n",
    "label_actual = emails['label'].to_numpy()\n",
    "label_pred = emails['predicted label'].to_numpy()\n",
    "\n",
    "#For Accuracy\n",
    "#Round to 2 decimal places and convert to percentage\n",
    "accuracy_result = accuracy_score(label_actual, label_pred)\n",
    "accuracy_result_p = round(accuracy_result * 100, 2)\n",
    "\n",
    "#For Recall\n",
    "recall_result = recall_score(label_actual, label_pred, pos_label=\"ham\")\n",
    "recall_result_p = round(recall_result * 100, 2)\n",
    "\n",
    "#For Precision\n",
    "precision_result = precision_score(label_actual, label_pred, pos_label=\"ham\")\n",
    "precision_result_p = round(precision_result *100, 2)\n",
    "\n",
    "#Results\n",
    "print(\"Performance Evaluation:\\n\")\n",
    "print(f\"The Accuracy Score is {accuracy_result} or {accuracy_result_p}%\")\n",
    "print(f\"The Recall Score is {recall_result} or {recall_result_p}%\")\n",
    "print(f\"The Precision Score is {precision_result} or {precision_result_p}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66a7dfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion Matrix:\n",
      "\n",
      "[[ 1393 11517]\n",
      " [  348 24564]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "confusion_mat = confusion_matrix(label_actual,label_pred)\n",
    "print(\"The Confusion Matrix:\\n\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a76c2a",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
